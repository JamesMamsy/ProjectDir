# -*- coding: utf-8 -*-
"""Apriori_eb.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Bp_6xCtC1zaD2wKbwBSjzW0awSC3QNki
"""

#!git clone https://github.com/JamesMamsy/ProjectDir

import os
import glob
import pandas as pd

def pickle_files(filetype, outfile, folder="/content/drive/MyDrive/OU/Data Mining/Data Files 2"):
    path = folder
    files = glob.glob(os.path.join(path, filetype))

    df_list = []
    for file in files:
        try:
            df = pd.read_csv(file)
            df_list.append(df)
        except Exception as e:
            print(f"Error reading {file}: {e}")

    if df_list:
        combined_df = pd.concat(df_list, ignore_index=True)
        combined_df.to_pickle(outfile)
    else:
        print("No files were processed.")

# Usage
pickle_files("*.csv", "output.pkl")  # Adjust filetype and output file name as needed

import os
import glob
import pandas as pd
import numpy as np

# Load the carrier names
carriers_path = "/content/drive/MyDrive/OU/Data Mining/Supplementary Data Files/L_UNIQUE_CARRIERS.csv"
carriers_df = pd.read_csv(carriers_path)

# Load the pickled DataFrame
df = pd.read_pickle('output.pkl')

# Define delay types
delay_types = ['CARRIER_DELAY', 'WEATHER_DELAY', 'NAS_DELAY', 'SECURITY_DELAY', 'LATE_AIRCRAFT_DELAY']

# Add 'No_Delay' column, which is 1 if all delay columns are NaN, otherwise 0
df['No_Delay'] = df[delay_types].isnull().all(axis=1).astype(int)

# Merge with carriers_df to get the full carrier name
df = df.merge(carriers_df, how='left', left_on='OP_UNIQUE_CARRIER', right_on='Code')

# Optionally, if 'Description' is the column with full carrier names
df.rename(columns={'Description': 'Carrier_Full_Name'}, inplace=True)

# Convert 'FL_DATE' to datetime if it's not already
df['FL_DATE'] = pd.to_datetime(df['FL_DATE'])

# Create a new column with the month in two digits
df['Month'] = df['FL_DATE'].dt.strftime('%m')

print(df.head())

# Get user input for month/day and carrier
user_month = input("Enter the month of the flight (MM): ")
user_carrier = input("Enter the carrier code: ")
user_dept_apt = input("Enter the 3 digit Departure Airport Code: ")

# Filter the DataFrame for the given month, carrier, and departure airport
filtered_df = df[(pd.to_datetime(df['FL_DATE']).dt.month == int(user_month)) &
                 (df['OP_UNIQUE_CARRIER'] == user_carrier) &
                 (df['ORIGIN'] == user_dept_apt)]

# Display the filtered DataFrame
print(filtered_df)

# Apply the binning to each delay column
for delay_type in delay_types:
    # Create a new column for the binned data
    bucket_col_name = delay_type + '_Bucket'
    # Replace zero delays with NaN, then bin the data
    filtered_df[bucket_col_name] = pd.cut(
        filtered_df[delay_type].replace(0, np.nan), bins=bins, labels=labels, include_lowest=True
    )

# List of bucket columns
delay_bucket_cols = ['CARRIER_DELAY_Bucket', 'WEATHER_DELAY_Bucket', 'NAS_DELAY_Bucket', 'SECURITY_DELAY_Bucket', 'LATE_AIRCRAFT_DELAY_Bucket']

# Create a new column based on the condition
filtered_df['Delay_Status'] = filtered_df.apply(
    lambda row: 'No Delay' if row[delay_bucket_cols].isnull().all() else 'Delay',
    axis=1
)

# Now, filtered_df has a new column 'Delay_Status' indicating whether there's a delay or not
# Replace NaN values with "No Delay" in the bucket columns
for delay_bucket_col in delay_bucket_cols:
    # Get the current categories
    current_categories = filtered_df[delay_bucket_col].cat.categories.tolist()

    # If "No Delay" is not already a category, add it
    if "No Delay" not in current_categories:
        new_categories = current_categories + ["No Delay"]
        filtered_df[delay_bucket_col].cat.set_categories(new_categories, inplace=True)

    # Now you can fill NaN values with "No Delay"
    filtered_df[delay_bucket_col].fillna('No Delay', inplace=True)

print(filtered_df.head())

other_relevant_cols = ['Month', 'Carrier_Full_Name','Delay_Status']
all_relevant_cols = delay_bucket_cols + other_relevant_cols

trim_df = filtered_df[all_relevant_cols]

print(trim_df.head())

# calculate probability of delay
# Count the number of rows with 'Delay_Status' as 'Delay'
delay_count = (filtered_df['Delay_Status'] == 'Delay').sum()

# Get the total number of rows in the DataFrame
total_rows = len(filtered_df)

# Calculate the proportion
delay_proportion = delay_count / total_rows * 100

print(f"Proportion of delays: {delay_proportion:.2f}%")

# Filter the DataFrame to include only rows with 'Delay' in 'Delay_Status'
delay_only_df = filtered_df[filtered_df['Delay_Status'] == 'Delay']

# Create a transformed dataset
transformed_data = []
for _, row in delay_only_df.iterrows():
    features = set()

    # Add concatenated delay type and bucket
    for delay_type in delay_types:
        bucket_col = delay_type + '_Bucket'
        delay_value = row[bucket_col]

        # Add delay feature only if it's not 'No Delay'
        if delay_value != 'No Delay':
            features.add(f"{delay_type}: {delay_value}")

    transformed_data.append(list(features))

# Apply TransactionEncoder
te = TransactionEncoder()
te_ary = te.fit(transformed_data).transform(transformed_data)
apriori_df = pd.DataFrame(te_ary, columns=te.columns_)

# Applying Apriori
frequent_itemsets = apriori(apriori_df, min_support=0.01, use_colnames=True)

# Print the frequent itemsets
print(frequent_itemsets)

frequent_itemsets.to_csv("frequent_itemsets.csv", index=False)

# Sort by support in descending order
frequent_itemsets_sorted = frequent_itemsets.sort_values('support', ascending=False)

# Get the highest support itemset
highest_support_itemset = frequent_itemsets_sorted.iloc[0]

# Parse out the value(s) from the itemset
highest_support_value = list(highest_support_itemset['itemsets'])[0]

# Print the highest support value and its support
print(f"Probability of Delay: {delay_proportion:.2f}%")
print("Most Likely Delay:", highest_support_value)